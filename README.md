
# MindLink AI - Intelligent Multi-Agent Customer Support Assistant

**MindLink AI** is a cutting-edge multi-agent customer support system designed to enhance and automate customer service interactions. Developed as the capstone project for the **Google Ã— Kaggle 5-Day AI Agents Intensive Course** by **Ashutosh Shukla**, this system provides efficient, intelligent, and context-aware support for an imaginary company, XYZ Company.

It integrates advanced **multi-agent architecture**, **LLM-powered replies**, **dynamic escalation mechanisms**, and **real-time observability**, all aimed at providing a seamless and responsive user experience.

---

## Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Architecture](#architecture)
- [Tech Stack](#tech-stack)
- [Installation & Setup](#installation--setup)
- [Deployment](#deployment)
- [Agent Evaluation](#agent-evaluation)
- [Repository Structure](#repository-structure)
- [License](#license)
- [Acknowledgements](#acknowledgements)

---

## Overview

MindLink AI simulates a real-world, enterprise-level customer service experience, featuring several intelligent agents that handle tasks such as:

- **Intent Detection**: Identifying user intent (e.g., billing, refund, cancellation).
- **Urgency Detection**: Analyzing and categorizing the urgency of requests.
- **Response Generation**: Generating professional, context-aware replies using the **Groq Llama-3.3-70B** LLM.
- **Escalation Handling**: Deciding when a request requires human intervention.
- **Safety Enforcement**: Blocking PII and inappropriate content.

The system includes **memory capabilities**, retaining conversation context and logs for future reference.

---

## Features

### ðŸ§  Multi-Agent System

- **IntentAgent** â€” Detects customer intent (e.g., billing, refund, cancellation).
- **UrgencyAgent** â€” Detects the urgency level of requests.
- **ReplyAgent** â€” Uses the **Groq Llama-3.3-70B** LLM to generate contextual replies.
- **PolicyAgent** â€” Enforces safety and PII policies.
- **EscalationAgent** â€” Determines when to escalate to a human.
- **SessionMemoryAgent** â€” Manages session memory.

### ðŸ”§ Tool Integration

- Custom tools (e.g., `lookup_order(order_id)`) for fetching real-time data like order details.

### ðŸ§¬ Memory System

- Short-term memory for active sessions and long-term persistent storage for user/context retention.

### ðŸ“Š Observability

- Metrics for latency, error/request counters, and detailed logs for each LLM call.

### ðŸ§ª Agent Evaluation

- Automated testing for agent latency, correctness, policy compliance, and escalation.

---

## Architecture

```
MindLink AI
â”‚
â”œâ”€â”€ frontend (React + Vite + Tailwind)
â”‚     â”œâ”€â”€ Chat UI (chat bubbles)
â”‚     â”œâ”€â”€ Avatar icons
â”‚     â”œâ”€â”€ Glassmorphic design
â”‚     â””â”€â”€ REST API â†’ backend
â”‚
â”œâ”€â”€ backend (FastAPI)
â”‚     â”œâ”€â”€ /api/ask endpoint
â”‚     â”œâ”€â”€ Multi-agent pipeline
â”‚     â”œâ”€â”€ GroqReplyAgent (LLM)
â”‚     â”œâ”€â”€ Policy & escalation agents
â”‚     â”œâ”€â”€ Custom tools (e.g., order lookup)
â”‚     â””â”€â”€ Metrics + memory storage
â”‚
â””â”€â”€ Groq Llama-3.3-70B (LLM engine)
```

---

## Tech Stack

### Frontend

- React, Vite, TailwindCSS, Framer-motion (animations)

### Backend

- FastAPI, Uvicorn, Python 3.10+, python-dotenv, Groq Llama integration

### Tools & Infra

- Custom tools, session & memory services, logging/metrics

---

## Installation & Setup

### 1. Clone the repository

```bash
git clone https://github.com/Ashu-Shukla-1309/MindLink-AI.git
cd MindLink-AI
```

### 2. Backend Setup

```bash
cd backend
python -m venv venv
# Windows
.\venv\Scripts\activate
# macOS / Linux
source venv/bin/activate
pip install -r requirements.txt
# create .env.local with your GROQ_API_KEY and GROQ_MODEL
uvicorn app:app --reload --port 8000
```

### 3. Frontend Setup

```bash
cd frontend
npm install
npm run dev
# Open http://localhost:5173
```

---

## Deployment

- **Frontend**: Deploy on Vercel (build with `npm run build`).
- **Backend**: Deploy on Render, Railway, or any containerized environment. Start with `uvicorn app:app --host 0.0.0.0 --port $PORT`.
- Set `VITE_BACKEND_URL` in the frontend `.env` file to point to your deployed backend.

---

## Agent Evaluation

The repository includes automated evaluation scripts to test:

- Latency & throughput
- Correctness of replies
- Policy enforcement
- Escalation triggers

---

## Repository Structure

Hereâ€™s an overview of the folder structure and the purpose of each major file/folder in the repository:

```
MindLink-AI/
â”œâ”€ .git/                         # Git metadata (hooks, logs, objects)
â”œâ”€ backend/                       # Python backend (agents, reply logic, tools)
â”‚  â”œâ”€ app.py                      # FastAPI entrypoint
â”‚  â”œâ”€ agents/                     # Multi-agent orchestration and logic
â”‚  â”œâ”€ evaluation/                 # Automated agent evaluation
â”‚  â”œâ”€ observability/              # Metrics helpers
â”‚  â”œâ”€ reply/                      # LLM integration
â”‚  â””â”€ tools/                      # Custom tools callable by agents
â”œâ”€ frontend/                      # React frontend (UI, chat components)
â”‚  â”œâ”€ index.html                  # HTML entrypoint
â”‚  â”œâ”€ package.json                # npm scripts & dependencies
â”‚  â””â”€ src/                        # React components & styles
â”œâ”€ LICENSE                        # MIT License
â”œâ”€ README.md                      # Project overview (this file)
â”œâ”€ start_app.bat                  # Windows helper to start both backend & frontend
â””â”€ structure.txt                  # Listing of project structure
```

---

## License

This project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for details.

---

## Acknowledgements

- **Google** & **Kaggle AI Agents Team** â€” Course & mentorship.
- **Groq** â€” LLM model used for generating intelligent responses.
- **FastAPI** â€” High-performance backend framework.
- **Vite + React** â€” Modern frontend tooling.
